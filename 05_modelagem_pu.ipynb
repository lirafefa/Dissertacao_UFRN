{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Nosso Conjunto de Dados: Cumprimento de Sentença em Ações Coletivas vs Demais Processos\n",
        "\n",
        "### Após a coleta inicial, foi selecionado um subconjunto de 3.000 registros para rotulagem manual, originando o DataFrame `df_rotulados`. Cada registro foi classificado com base em sua natureza processual, distinguindo entre:\n",
        "\n",
        "- **Cumprimento de sentença em ações coletivas**, representado pelo valor **1**\n",
        "- **Demais tipos de processos**, representados pelo valor **0**\n",
        "\n",
        "Essa conversão para valores numéricos foi essencial para possibilitar análises quantitativas e treinamentos de modelos de machine learning posteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opG6Ib79-lx1",
        "outputId": "c9c67b2c-711f-40dc-d980-412f89416e7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Primeiras linhas do DataFrame original:\n",
            "['Negative' 'Positive' 'Neutral' nan]\n",
            "Arquivo filtrado salvo com sucesso: dados/dados_rotulados_filtrados.csv\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import pandas as pd\n",
        "load_dotenv()\n",
        "\n",
        "dfs = []\n",
        "path_dados = os.getenv(\"PATH_DADOS\")\n",
        "count = 0\n",
        "for arquivo in os.listdir(path_dados):\n",
        "    # Só foi nescessário pegar 5 meses aleatório pois eles já teriam 10000 casos\n",
        "    if arquivo.endswith('.parquet') and count<5:\n",
        "      dfs.append(pd.read_parquet(f'{path_dados}/{arquivo}'))\n",
        "      count=+1\n",
        "\n",
        "\n",
        "SEED = 130397\n",
        "\n",
        "df = pd.concat(dfs,ignore_index=True)\n",
        "df_rotulados = pd.read_csv(f'{path_dados}/dados_rotulados.csv')\n",
        "\n",
        "# Caminho correto dos dados rotulados\n",
        "arquivo_entrada = f'{path_dados}/dados_rotulados.csv'\n",
        "arquivo_saida = f'{path_dados}/dados_rotulados_filtrados.csv'\n",
        "\n",
        "# Verifica se o arquivo existe antes de continuar\n",
        "if not os.path.exists(arquivo_entrada):\n",
        "    print(f\"Erro: O arquivo '{arquivo_entrada}' não foi encontrado. Verifique o caminho!\")\n",
        "else:\n",
        "    # Carregando os dados rotulados\n",
        "    df_rotulados = pd.read_csv(arquivo_entrada,index_col=0).reset_index(drop=True)\n",
        "\n",
        "    # Exibe as primeiras linhas para entender o formato\n",
        "    print(\"Primeiras linhas do DataFrame original:\")\n",
        "    #print(df_rotulados.head())\n",
        "\n",
        "    print(df_rotulados.sentiment.unique())\n",
        "\n",
        "    # Dicionário de mapeamento para converter sentimentos em valores numéricos\n",
        "    mapeamento_sentimento = {\n",
        "        'Positive': 1,\n",
        "        'Negative': 0,\n",
        "    }\n",
        "\n",
        "    # Verifica se a coluna 'sentiment' existe no DataFrame\n",
        "    if 'sentiment' not in df_rotulados.columns:\n",
        "        print(\"Erro: A coluna 'sentiment' não existe no DataFrame.\")\n",
        "    else:\n",
        "        # Aplicando o mapeamento na coluna 'sentiment'\n",
        "        df_rotulados['sentiment'] = df_rotulados['sentiment'].map(mapeamento_sentimento)\n",
        "\n",
        "        # Removendo valores neutros (0) e não classificados (NaN)\n",
        "        df_rotulados = df_rotulados.dropna(subset=['sentiment'])\n",
        "\n",
        "        # Convertendo a coluna 'sentiment' para inteiro (opcional)\n",
        "        df_rotulados['sentiment'] = df_rotulados['sentiment'].astype(int)\n",
        "\n",
        "        # Salvando o novo arquivo filtrado\n",
        "        df_rotulados.to_csv(arquivo_saida, index=False)\n",
        "\n",
        "        print(f\"Arquivo filtrado salvo com sucesso: {arquivo_saida}\")\n",
        "\n",
        "df_rotulados = df_rotulados.rename(columns={'sentiment':'Cumprimento_Sentenca'})\n",
        "df_rotulados_positivo  = df_rotulados[df_rotulados['Cumprimento_Sentenca']==1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Divisão dos Dados: Estratégia Holdout\n",
        "\n",
        "Nesta etapa, aplicamos a técnica de **Holdout** para dividir os dados em três conjuntos:\n",
        "\n",
        "- **Treinamento (80%)**: usado para ajustar os modelos.\n",
        "- **Validação (10%)**: utilizado para ajustar hiperparâmetros e evitar overfitting.\n",
        "- **Teste (10%)**: reservado para a avaliação final do desempenho do modelo.\n",
        "\n",
        "Essa divisão garante uma separação clara entre os dados usados para aprendizado e os usados para avaliação, contribuindo para a robustez e generalização dos resultados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4Ds5_KZ-lx6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_holdout(df):\n",
        "    \"\"\"\n",
        "    Separa um dataframe em conjuntos de treino (80%), validação (10%) e teste (10%).\n",
        "\n",
        "    Parâmetros:\n",
        "    df (DataFrame): O dataframe contendo os dados não rotulados.\n",
        "\n",
        "    Retorna:\n",
        "    tuple: (treino, validacao, teste)\n",
        "    \"\"\"\n",
        "\n",
        "    # Separar 80% para treino e 20% para validação + teste\n",
        "    treino, temp = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Separar 10% para validação e 10% para teste\n",
        "    validacao, teste = train_test_split(temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    return treino, validacao, teste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extração de Características com BERT\n",
        "\n",
        "Nesta etapa, utilizamos o modelo **ModernBERT-base** da Hugging Face para transformar os textos jurídicos em **vetores numéricos de características** (embeddings), que serão usados como entrada para modelos de machine learning.\n",
        "\n",
        "O processo é realizado da seguinte forma:\n",
        "\n",
        "- Cada texto é tokenizado e processado por um modelo pré-treinado da arquitetura BERT.\n",
        "- A saída da última camada oculta é utilizada para gerar um vetor de características representando semanticamente o texto.\n",
        "- O vetor resultante é obtido pela média dos embeddings de cada token, garantindo uma representação compacta e significativa.\n",
        "- As características extraídas são combinadas com seus respectivos rótulos (`Cumprimento_Sentenca`) para posterior treinamento e avaliação.\n",
        "\n",
        "Essa etapa é aplicada a todos os conjuntos de dados: **treinamento**, **validação** e **teste**, tanto para os exemplos rotulados quanto para os não rotulados.\n",
        "\n",
        "Além disso, os resultados são armazenados em arquivos `.parquet`, evitando a reprocessamento desnecessário e otimizando o tempo de execução nos ciclos futuros de experimentação.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Vk_vvYQPbqwU",
        "outputId": "9f72e958-d3eb-41e1-dc9e-8040087fb15f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "def extrair_caracteristicas_de_dataframe(\n",
        "    df: pd.DataFrame, tokenizer: AutoTokenizer, modelo: AutoModel\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Extrai características de um DataFrame contendo textos e rótulos.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame contendo as colunas 'texto' e 'Cumprimento_Sentença'.\n",
        "        tokenizer (AutoTokenizer): O tokenizer do Hugging Face.\n",
        "        modelo (AutoModel): O modelo pré-treinado do Hugging Face.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame com colunas ['rotulo', 'texto', 'caracteristicas']\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    modelo.to(device)\n",
        "    caracteristicas = []\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "        texto = row[\"texto\"]\n",
        "        rotulo = row.get(\"Cumprimento_Sentenca\", 0)  # Assume 0 se não houver valor\n",
        "\n",
        "        if pd.isna(texto):\n",
        "            continue  # Ignorar linhas sem texto\n",
        "\n",
        "        # Tokenizar o texto\n",
        "        entradas = tokenizer(texto, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "        entradas = {k: v.to(device) for k, v in entradas.items()}  # Mover inputs para a GPU\n",
        "\n",
        "        # Passar o texto tokenizado pelo modelo para obter as saídas\n",
        "        with torch.no_grad():\n",
        "            saidas = modelo(**entradas)\n",
        "\n",
        "        # Extrair os embeddings da última camada do modelo\n",
        "        ultimos_estados_ocultos = saidas.last_hidden_state\n",
        "\n",
        "        # Calcular a média dos estados ocultos para obter um vetor de características\n",
        "        vetor_de_caracteristicas = (\n",
        "            ultimos_estados_ocultos.mean(axis=1).squeeze().detach().cpu().numpy()\n",
        "        )\n",
        "\n",
        "        # Adicionar à lista de características\n",
        "        caracteristicas.append((rotulo, texto, vetor_de_caracteristicas))\n",
        "\n",
        "    return pd.DataFrame(caracteristicas, columns=[\"rotulo\", \"texto\", \"caracteristicas\"])\n",
        "\n",
        "\n",
        "# Amostragem e divisão dos DataFrames\n",
        "tamanho = 10000\n",
        "df_nao_rotulado = df.sample(tamanho - len(df_rotulados_positivo))\n",
        "cumprimento_treino, cumprimento_validacao, cumprimento_teste = split_holdout(df_rotulados_positivo)\n",
        "nao_rotulado_treino, nao_rotulado_validacao, nao_rotulado_teste = split_holdout(df_nao_rotulado)\n",
        "\n",
        "# Carregar o modelo e o tokenizer do Hugging Face para o processamento de texto\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
        "modelo = AutoModel.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
        "\n",
        "if 'caracteristicas_treino' in os.listdir():\n",
        "  caracteristicas_treino = pd.read_parquet('caracterisiticas_treino.parquet')\n",
        "else:\n",
        "  caracteristicas_treino = pd.concat([\n",
        "      extrair_caracteristicas_de_dataframe(cumprimento_treino, tokenizer, modelo),\n",
        "      extrair_caracteristicas_de_dataframe(nao_rotulado_treino, tokenizer, modelo)\n",
        "  ])\n",
        "if 'caracteristicas_validacao.parquet' in os.listdir():\n",
        "  caracteristicas_validacao = pd.read_parquet('caracterisiticas_validacao.parquet')\n",
        "else:\n",
        "  caracteristicas_validacao = pd.concat([\n",
        "    extrair_caracteristicas_de_dataframe(cumprimento_validacao, tokenizer, modelo),\n",
        "    extrair_caracteristicas_de_dataframe(df_rotulados[df_rotulados['Cumprimento_Sentenca']==0],tokenizer, modelo),\n",
        "])\n",
        "if 'caracteristicas_teste.parquet' in os.listdir():\n",
        "  caracteristicas_teste = pd.read_parquet('caracterisiticas_teste.parquet')\n",
        "else:\n",
        "  caracteristicas_teste = pd.concat([\n",
        "      extrair_caracteristicas_de_dataframe(cumprimento_teste, tokenizer, modelo),\n",
        "      extrair_caracteristicas_de_dataframe(nao_rotulado_teste, tokenizer, modelo)\n",
        "  ])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##Salvamento dos Dados Vetorizados\n",
        "\n",
        "Após a extração das características com o modelo BERT, os conjuntos de **treinamento**, **validação** e **teste** foram reorganizados com `reset_index` para garantir consistência nos índices e, em seguida, salvos em formato `.parquet`.\n",
        "\n",
        "Esse formato foi escolhido por sua eficiência em leitura e escrita de grandes volumes de dados, facilitando o reuso dos vetores sem a necessidade de reprocessamento futuro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTSut1RNY85m"
      },
      "outputs": [],
      "source": [
        "caracterisiticas_treino = caracteristicas_treino.reset_index(drop=True)\n",
        "caracterisiticas_validacao = caracteristicas_validacao.reset_index(drop=True)\n",
        "caracterisiticas_teste = caracteristicas_teste.reset_index(drop=True)\n",
        "\n",
        "caracteristicas_treino.to_parquet('caracterisiticas_treino.parquet')\n",
        "caracteristicas_validacao.to_parquet('caracterisiticas_validacao.parquet')\n",
        "caracteristicas_teste.to_parquet('caracterisiticas_teste.parquet')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualização das Características com PCA (3D)\n",
        "\n",
        "Para compreender melhor a separabilidade entre as classes com base nos vetores gerados pelo modelo BERT, aplicamos **Análise de Componentes Principais (PCA)** com 3 componentes.\n",
        "\n",
        "### O que foi feito:\n",
        "\n",
        "- A coluna de características vetorizadas foi convertida para um array NumPy.\n",
        "- Aplicamos **PCA** para reduzir a dimensionalidade dos vetores de alta dimensão para apenas **3 componentes principais**, preservando o máximo possível da variância original.\n",
        "- Geramos um **gráfico de dispersão 3D** com os vetores projetados, colorindo os pontos de acordo com seus rótulos:\n",
        "  - `Rótulo 1` (Cumprimento de Sentença de Ação Coletiva)\n",
        "  - `Rótulo 0` (Demais Processos)\n",
        "\n",
        "### Objetivo:\n",
        "\n",
        "Essa visualização permite uma inspeção visual da separabilidade entre as classes no espaço vetorial, podendo indicar se os embeddings possuem potencial discriminativo para tarefas de classificação.\n",
        "\n",
        "A transparência (`alpha=0.02`) foi aplicada aos pontos do rótulo 0 para facilitar a visualização da densidade de pontos positivos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJqjSt7McgSe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Carregar os dados\n",
        "caracteristicas_treino = pd.read_parquet('caracterisiticas_treino.parquet')\n",
        "\n",
        "# Converter a coluna 'caracteristicas' para um array NumPy\n",
        "caracteristicas = np.array(caracteristicas_treino['caracteristicas'].tolist())\n",
        "\n",
        "# Aplicar PCA para reduzir a dimensionalidade para 3 componentes\n",
        "pca = PCA(n_components=3)\n",
        "caracteristicas_pca = pca.fit_transform(caracteristicas)\n",
        "\n",
        "# Imprimir a variância explicada\n",
        "print(\"Variância explicada por cada componente:\", pca.explained_variance_ratio_)\n",
        "print(\"Variância total explicada:\", np.sum(pca.explained_variance_ratio_))\n",
        "\n",
        "# Criar o gráfico 3D de dispersão\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "ax.scatter(caracteristicas_pca[caracteristicas_treino['rotulo'] == 1, 0],\n",
        "           caracteristicas_pca[caracteristicas_treino['rotulo'] == 1, 1],\n",
        "           caracteristicas_pca[caracteristicas_treino['rotulo'] == 1, 2],\n",
        "           label='Rótulo 1', marker='x')\n",
        "ax.scatter(caracteristicas_pca[caracteristicas_treino['rotulo'] == 0, 0],\n",
        "           caracteristicas_pca[caracteristicas_treino['rotulo'] == 0, 1],\n",
        "           caracteristicas_pca[caracteristicas_treino['rotulo'] == 0, 2],\n",
        "           label='Rótulo 0', marker='o',alpha=0.02)\n",
        "\n",
        "ax.set_xlabel('Componente Principal 1')\n",
        "ax.set_ylabel('Componente Principal 2')\n",
        "ax.set_zlabel('Componente Principal 3')\n",
        "ax.set_title('PCA das Características em 3D')\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Formatação e Análise dos Rótulos\n",
        "\n",
        "Nesta etapa, os rótulos do conjunto de treino foram convertidos para o formato esperado por modelos de aprendizado semi-supervisionado:\n",
        "\n",
        "- Rótulo **1**: representa um exemplo **positivo** (cumprimento de sentença).\n",
        "- Rótulo **-1**: representa um exemplo **não rotulado** (demais processos, tratados como negativos ou desconhecidos).\n",
        "\n",
        "Como a função `np.bincount()` não aceita valores negativos, foi aplicada uma transformação temporária, somando **1** a cada rótulo, para deslocar os valores ao intervalo não-negativo:\n",
        "\n",
        "- O valor `-1` se torna `0` (primeira posição do array de contagens).\n",
        "- O valor `1` se torna `2` (terceira posição).\n",
        "\n",
        "Esse processo permitiu contar de forma correta e eficiente quantos exemplos rotulados e não rotulados estavam presentes no conjunto de treino."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHIPEOMpeIAq"
      },
      "outputs": [],
      "source": [
        "X_train = caracteristicas_treino['caracteristicas']\n",
        "y_train = caracteristicas_treino['rotulo'].tolist()\n",
        "X_valid = caracteristicas_validacao['caracteristicas']\n",
        "y_valid = caracteristicas_validacao['rotulo'].tolist()\n",
        "\n",
        "\n",
        "# Converter os rótulos combinados para um novo formato\n",
        "# Se o rótulo for 0 (não rotulado), converter para -1\n",
        "# Se o rótulo for 1 (positivo), manter como 1\n",
        "y_train_formatted = np.array([-1 if x == 0 else 1 for x in y_train])\n",
        "\n",
        "# Contar as ocorrências de cada rótulo (-1 e 1) nos rótulos formatados\n",
        "# np.bincount conta o número de ocorrências de cada valor em um array\n",
        "# Como o np.bincount espera apenas inteiros não negativos, ele não funciona diretamente com o valor -1\n",
        "# Para contornar isso, podemos somar 1 a cada elemento antes de contar\n",
        "# Isso desloca os valores para o intervalo de inteiros não negativos\n",
        "counts = np.bincount(y_train_formatted + 1)\n",
        "\n",
        "# Imprimir as contagens de -1 e 1\n",
        "# counts[0] corresponde à contagem de -1 (originalmente 0 no intervalo deslocado)\n",
        "# counts[2] corresponde à contagem de 1 (originalmente 2 no intervalo deslocado)\n",
        "print(f\"Count of -1 (unlabeled): {counts[0]}\")\n",
        "print(f\"Count of 1 (positive): {counts[2]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZAT0TMPm-wD"
      },
      "outputs": [],
      "source": [
        "X_train = np.array(X_train.tolist())\n",
        "X_valid = np.array(X_valid.tolist())\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_valid = np.array(y_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aprendizado Positivo-Não Rotulado com Elkan e Noto (PU Learning)\n",
        "\n",
        "Nesta etapa, aplicamos a técnica de **PU Learning (Positive-Unlabeled Learning)** utilizando a abordagem proposta por **Elkan e Noto (2008)** para treinar um classificador a partir de um conjunto de dados contendo apenas exemplos **positivos rotulados** e **exemplos não rotulados** (que podem ser positivos ou negativos).\n",
        "\n",
        "A implementação foi feita com a biblioteca `pulearn`, que oferece suporte nativo à técnica, estendendo os classificadores do `scikit-learn`.\n",
        "\n",
        "### Abordagem de Elkan e Noto: Etapas Principais\n",
        "\n",
        "1. **Treinamento de um Classificador Inicial**  \n",
        "   Treinamos um modelo probabilístico (neste caso, um `MLPClassifier`) para estimar a probabilidade de uma amostra ter sido **rotulada**.  \n",
        "\n",
        "2. **Estimativa de Probabilidade de Rotulagem**  \n",
        "   Após o treinamento, reservamos uma fração dos exemplos positivos para estimar a probabilidade \\\\( P(s = 1 \\mid y = 1) \\\\), ou seja, a probabilidade de um exemplo verdadeiramente positivo ter sido rotulado.\n",
        "\n",
        "3. **Ajuste das Probabilidades Estimadas**  \n",
        "   Para cada exemplo **não rotulado**, o modelo estima a probabilidade de ele ter sido rotulado.  \n",
        "   Essa probabilidade é então **corrigida** dividindo-a por \\\\( P(s = 1 \\mid y = 1) \\\\), permitindo estimar com mais precisão se a amostra **é de fato positiva**.\n",
        "\n",
        "### Implementação com `pulearn`\n",
        "\n",
        "Utilizamos o `ElkanotoPuClassifier` da biblioteca `pulearn`, com os seguintes parâmetros:\n",
        "\n",
        "- **Estimador base**: `MLPClassifier`, um classificador neural do `sklearn`.\n",
        "- **`hold_out_ratio=0.10`**: 10% das amostras positivas foram reservadas para estimar a probabilidade de rotulagem.\n",
        "- **Rótulos de entrada**:\n",
        "  - `1` para exemplos **positivos rotulados**.\n",
        "  - `-1` para exemplos **não rotulados**.\n",
        "\n",
        "### Vantagens da Abordagem\n",
        "- Corrige a **rotulagem incompleta** presente nos dados PU.\n",
        "- Permite treinar modelos mesmo **sem exemplos negativos rotulados**.\n",
        "- Ideal para cenários onde é difícil obter exemplos negativos confiáveis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCT1d-Pyhz5L"
      },
      "outputs": [],
      "source": [
        "from pulearn import ElkanotoPuClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, matthews_corrcoef\n",
        "import helpers.classification\n",
        "\n",
        "# Inicializar o RandomForestClassifier\n",
        "# n_jobs=-1: Utiliza todos os núcleos de CPU disponíveis para processamento paralelo\n",
        "# random_state=271828: Semente para o gerador de números aleatórios, garantindo reprodutibilidade\n",
        "model = MLPClassifier(random_state=271828)\n",
        "\n",
        "# Inicializar o ElkanotoPuClassifier com o RandomForestClassifier como estimador base\n",
        "# hold_out_ratio=0.30: Proporção de amostras positivas reservadas para estimar P(s=1|y=1)\n",
        "pu_estimator = ElkanotoPuClassifier(estimator=model, hold_out_ratio=0.10)\n",
        "\n",
        "# Treinar o classificador PU no conjunto de dados combinado\n",
        "# X_train: Matriz de características contendo amostras positivas e não rotuladas\n",
        "# y_train_formatted: Rótulos formatados como -1 para não rotuladas e 1 para amostras positivas\n",
        "pu_estimator.fit(X_train, y_train_formatted)\n",
        "\n",
        "# Prever os rótulos para o conjunto de validação usando o classificador PU treinado\n",
        "y_valid_pred = pu_estimator.predict(X_valid)\n",
        "\n",
        "# Exibir as métricas de classificação\n",
        "helpers.classification.print_classification_metrics(y_valid, y_valid_pred)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
