{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mudança"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opG6Ib79-lx1",
        "outputId": "c9c67b2c-711f-40dc-d980-412f89416e7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Primeiras linhas do DataFrame original:\n",
            "['Negative' 'Positive' 'Neutral' nan]\n",
            "Arquivo filtrado salvo com sucesso: dados/dados_rotulados_filtrados.csv\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import pandas as pd\n",
        "load_dotenv()\n",
        "\n",
        "dfs = []\n",
        "path_dados = os.getenv(\"PATH_DADOS\")\n",
        "count = 0\n",
        "for arquivo in os.listdir(path_dados):\n",
        "    # Só foi nescessário pegar 5 meses aleatório pois eles já teriam 10000 casos\n",
        "    if arquivo.endswith('.parquet') and count<5:\n",
        "      dfs.append(pd.read_parquet(f'{path_dados}/{arquivo}'))\n",
        "      count=+1\n",
        "\n",
        "\n",
        "SEED = 130397\n",
        "\n",
        "df = pd.concat(dfs,ignore_index=True)\n",
        "df_rotulados = pd.read_csv(f'{path_dados}/dados_rotulados.csv')\n",
        "\n",
        "# Caminho correto dos dados rotulados\n",
        "arquivo_entrada = f'{path_dados}/dados_rotulados.csv'\n",
        "arquivo_saida = f'{path_dados}/dados_rotulados_filtrados.csv'\n",
        "\n",
        "# Verifica se o arquivo existe antes de continuar\n",
        "if not os.path.exists(arquivo_entrada):\n",
        "    print(f\"Erro: O arquivo '{arquivo_entrada}' não foi encontrado. Verifique o caminho!\")\n",
        "else:\n",
        "    # Carregando os dados rotulados\n",
        "    df_rotulados = pd.read_csv(arquivo_entrada,index_col=0).reset_index(drop=True)\n",
        "\n",
        "    # Exibe as primeiras linhas para entender o formato\n",
        "    print(\"Primeiras linhas do DataFrame original:\")\n",
        "    #print(df_rotulados.head())\n",
        "\n",
        "    print(df_rotulados.sentiment.unique())\n",
        "\n",
        "    # Dicionário de mapeamento para converter sentimentos em valores numéricos\n",
        "    mapeamento_sentimento = {\n",
        "        'Positive': 1,\n",
        "        'Negative': 0,\n",
        "    }\n",
        "\n",
        "    # Verifica se a coluna 'sentiment' existe no DataFrame\n",
        "    if 'sentiment' not in df_rotulados.columns:\n",
        "        print(\"Erro: A coluna 'sentiment' não existe no DataFrame.\")\n",
        "    else:\n",
        "        # Aplicando o mapeamento na coluna 'sentiment'\n",
        "        df_rotulados['sentiment'] = df_rotulados['sentiment'].map(mapeamento_sentimento)\n",
        "\n",
        "        # Removendo valores neutros (0) e não classificados (NaN)\n",
        "        df_rotulados = df_rotulados.dropna(subset=['sentiment'])\n",
        "\n",
        "        # Convertendo a coluna 'sentiment' para inteiro (opcional)\n",
        "        df_rotulados['sentiment'] = df_rotulados['sentiment'].astype(int)\n",
        "\n",
        "        # Salvando o novo arquivo filtrado\n",
        "        df_rotulados.to_csv(arquivo_saida, index=False)\n",
        "\n",
        "        print(f\"Arquivo filtrado salvo com sucesso: {arquivo_saida}\")\n",
        "\n",
        "df_rotulados = df_rotulados.rename(columns={'sentiment':'Cumprimento_Sentenca'})\n",
        "df_rotulados_positivo  = df_rotulados[df_rotulados['Cumprimento_Sentenca']==1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4Ds5_KZ-lx6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_holdout(df):\n",
        "    \"\"\"\n",
        "    Separa um dataframe em conjuntos de treino (80%), validação (10%) e teste (10%).\n",
        "\n",
        "    Parâmetros:\n",
        "    df (DataFrame): O dataframe contendo os dados não rotulados.\n",
        "\n",
        "    Retorna:\n",
        "    tuple: (treino, validacao, teste)\n",
        "    \"\"\"\n",
        "\n",
        "    # Separar 80% para treino e 20% para validação + teste\n",
        "    treino, temp = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Separar 10% para validação e 10% para teste\n",
        "    validacao, teste = train_test_split(temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    return treino, validacao, teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Vk_vvYQPbqwU",
        "outputId": "9f72e958-d3eb-41e1-dc9e-8040087fb15f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "def extrair_caracteristicas_de_dataframe(\n",
        "    df: pd.DataFrame, tokenizer: AutoTokenizer, modelo: AutoModel\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Extrai características de um DataFrame contendo textos e rótulos.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame contendo as colunas 'texto' e 'Cumprimento_Sentença'.\n",
        "        tokenizer (AutoTokenizer): O tokenizer do Hugging Face.\n",
        "        modelo (AutoModel): O modelo pré-treinado do Hugging Face.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame com colunas ['rotulo', 'texto', 'caracteristicas']\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    modelo.to(device)\n",
        "    caracteristicas = []\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "        texto = row[\"texto\"]\n",
        "        rotulo = row.get(\"Cumprimento_Sentenca\", 0)  # Assume 0 se não houver valor\n",
        "\n",
        "        if pd.isna(texto):\n",
        "            continue  # Ignorar linhas sem texto\n",
        "\n",
        "        # Tokenizar o texto\n",
        "        entradas = tokenizer(texto, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "        entradas = {k: v.to(device) for k, v in entradas.items()}  # Mover inputs para a GPU\n",
        "\n",
        "        # Passar o texto tokenizado pelo modelo para obter as saídas\n",
        "        with torch.no_grad():\n",
        "            saidas = modelo(**entradas)\n",
        "\n",
        "        # Extrair os embeddings da última camada do modelo\n",
        "        ultimos_estados_ocultos = saidas.last_hidden_state\n",
        "\n",
        "        # Calcular a média dos estados ocultos para obter um vetor de características\n",
        "        vetor_de_caracteristicas = (\n",
        "            ultimos_estados_ocultos.mean(axis=1).squeeze().detach().cpu().numpy()\n",
        "        )\n",
        "\n",
        "        # Adicionar à lista de características\n",
        "        caracteristicas.append((rotulo, texto, vetor_de_caracteristicas))\n",
        "\n",
        "    return pd.DataFrame(caracteristicas, columns=[\"rotulo\", \"texto\", \"caracteristicas\"])\n",
        "\n",
        "\n",
        "# Amostragem e divisão dos DataFrames\n",
        "tamanho = 10000\n",
        "df_nao_rotulado = df.sample(tamanho - len(df_rotulados_positivo))\n",
        "cumprimento_treino, cumprimento_validacao, cumprimento_teste = split_holdout(df_rotulados_positivo)\n",
        "nao_rotulado_treino, nao_rotulado_validacao, nao_rotulado_teste = split_holdout(df_nao_rotulado)\n",
        "\n",
        "# Carregar o modelo e o tokenizer do Hugging Face para o processamento de texto\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
        "modelo = AutoModel.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
        "\n",
        "if 'caracteristicas_treino' in os.listdir():\n",
        "  caracteristicas_treino = pd.read_parquet('caracterisiticas_treino.parquet')\n",
        "else:\n",
        "  caracteristicas_treino = pd.concat([\n",
        "      extrair_caracteristicas_de_dataframe(cumprimento_treino, tokenizer, modelo),\n",
        "      extrair_caracteristicas_de_dataframe(nao_rotulado_treino, tokenizer, modelo)\n",
        "  ])\n",
        "if 'caracteristicas_validacao.parquet' in os.listdir():\n",
        "  caracteristicas_validacao = pd.read_parquet('caracterisiticas_validacao.parquet')\n",
        "else:\n",
        "  caracteristicas_validacao = pd.concat([\n",
        "    extrair_caracteristicas_de_dataframe(cumprimento_validacao, tokenizer, modelo),\n",
        "    extrair_caracteristicas_de_dataframe(df_rotulados[df_rotulados['Cumprimento_Sentenca']==0],tokenizer, modelo),\n",
        "])\n",
        "if 'caracteristicas_teste.parquet' in os.listdir():\n",
        "  caracteristicas_teste = pd.read_parquet('caracterisiticas_teste.parquet')\n",
        "else:\n",
        "  caracteristicas_teste = pd.concat([\n",
        "      extrair_caracteristicas_de_dataframe(cumprimento_teste, tokenizer, modelo),\n",
        "      extrair_caracteristicas_de_dataframe(nao_rotulado_teste, tokenizer, modelo)\n",
        "  ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTSut1RNY85m"
      },
      "outputs": [],
      "source": [
        "caracterisiticas_treino = caracteristicas_treino.reset_index(drop=True)\n",
        "caracterisiticas_validacao = caracteristicas_validacao.reset_index(drop=True)\n",
        "caracterisiticas_teste = caracteristicas_teste.reset_index(drop=True)\n",
        "\n",
        "caracteristicas_treino.to_parquet('caracterisiticas_treino.parquet')\n",
        "caracteristicas_validacao.to_parquet('caracterisiticas_validacao.parquet')\n",
        "caracteristicas_teste.to_parquet('caracterisiticas_teste.parquet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJqjSt7McgSe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Carregar os dados\n",
        "caracteristicas_treino = pd.read_parquet('caracterisiticas_treino.parquet')\n",
        "\n",
        "# Converter a coluna 'caracteristicas' para um array NumPy\n",
        "caracteristicas = np.array(caracteristicas_treino['caracteristicas'].tolist())\n",
        "\n",
        "# Aplicar PCA para reduzir a dimensionalidade para 3 componentes\n",
        "pca = PCA(n_components=3)\n",
        "caracteristicas_pca = pca.fit_transform(caracteristicas)\n",
        "\n",
        "# Imprimir a variância explicada\n",
        "print(\"Variância explicada por cada componente:\", pca.explained_variance_ratio_)\n",
        "print(\"Variância total explicada:\", np.sum(pca.explained_variance_ratio_))\n",
        "\n",
        "# Criar o gráfico 3D de dispersão\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "ax.scatter(caracteristicas_pca[caracteristicas_treino['rotulo'] == 1, 0],\n",
        "           caracteristicas_pca[caracteristicas_treino['rotulo'] == 1, 1],\n",
        "           caracteristicas_pca[caracteristicas_treino['rotulo'] == 1, 2],\n",
        "           label='Rótulo 1', marker='x')\n",
        "ax.scatter(caracteristicas_pca[caracteristicas_treino['rotulo'] == 0, 0],\n",
        "           caracteristicas_pca[caracteristicas_treino['rotulo'] == 0, 1],\n",
        "           caracteristicas_pca[caracteristicas_treino['rotulo'] == 0, 2],\n",
        "           label='Rótulo 0', marker='o',alpha=0.02)\n",
        "\n",
        "ax.set_xlabel('Componente Principal 1')\n",
        "ax.set_ylabel('Componente Principal 2')\n",
        "ax.set_zlabel('Componente Principal 3')\n",
        "ax.set_title('PCA das Características em 3D')\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHIPEOMpeIAq"
      },
      "outputs": [],
      "source": [
        "X_train = caracteristicas_treino['caracteristicas']\n",
        "y_train = caracteristicas_treino['rotulo'].tolist()\n",
        "X_valid = caracteristicas_validacao['caracteristicas']\n",
        "y_valid = caracteristicas_validacao['rotulo'].tolist()\n",
        "\n",
        "\n",
        "# Convert the combined labels to a new format\n",
        "# If the label is 0 (unlabeled), convert it to -1\n",
        "# If the label is 1 (positive), keep it as 1\n",
        "y_train_formatted = np.array([-1 if x == 0 else 1 for x in y_train])\n",
        "\n",
        "# Count the occurrences of each label (-1 and 1) in the formatted labels\n",
        "# np.bincount counts the number of occurrences of each value in the array\n",
        "# Since np.bincount expects non-negative integers, it will not work directly with -1\n",
        "# To handle this, we can use a workaround by adding 1 to each element before counting\n",
        "# This shifts the range to non-negative integers\n",
        "counts = np.bincount(y_train_formatted + 1)\n",
        "\n",
        "# Print the counts of -1 and 1\n",
        "# counts[0] corresponds to the count of -1 (originally 0 in the shifted range)\n",
        "# counts[2] corresponds to the count of 1 (originally 2 in the shifted range)\n",
        "print(f\"Count of -1 (unlabeled): {counts[0]}\")\n",
        "print(f\"Count of 1 (positive): {counts[2]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZAT0TMPm-wD"
      },
      "outputs": [],
      "source": [
        "X_train = np.array(X_train.tolist())\n",
        "X_valid = np.array(X_valid.tolist())\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_valid = np.array(y_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCT1d-Pyhz5L"
      },
      "outputs": [],
      "source": [
        "from pulearn import ElkanotoPuClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, matthews_corrcoef\n",
        "import helpers.classification\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "# n_jobs=-1: Use all available CPU cores for parallel processing\n",
        "# random_state=271828: Seed for random number generator to ensure reproducibility\n",
        "model = MLPClassifier(random_state=271828)\n",
        "\n",
        "# Initialize the ElkanotoPuClassifier with the RandomForestClassifier as the base estimator\n",
        "# hold_out_ratio=0.30: Ratio of positive samples to hold out for estimating P(s=1|y=1)\n",
        "pu_estimator = ElkanotoPuClassifier(estimator=model, hold_out_ratio=0.10)\n",
        "\n",
        "# Fit the PU classifier on the combined dataset\n",
        "# X_train: Feature matrix containing both positive and unlabeled samples\n",
        "# y_train_formatted: Labels formatted to -1 for unlabeled and 1 for positive samples\n",
        "pu_estimator.fit(X_train, y_train_formatted)\n",
        "\n",
        "# Predict labels for the validation set using the trained PU classifier\n",
        "y_valid_pred = pu_estimator.predict(X_valid)\n",
        "\n",
        "# Show the classification metrics\n",
        "helpers.classification.print_classification_metrics(y_valid, y_valid_pred)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
